# -*- coding: utf-8 -*-
"""Research.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNPton7X0pEvGD8Wwr4n43VXHz6_Tggx
"""

#  Stroke Prediction Model


import pandas as pd
import numpy as np
import pickle
import json6

import sys
import os
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Data preprocessing functions
def load_dataset(file_path):
    """Load and perform basic validation of the dataset"""
    try:
        df = pd.read_csv(file_path)
        print(f"Dataset loaded successfully: {df.shape}")
        print(f"Columns: {list(df.columns)}")
        print(f"Target distribution:\n{df['stroke'].value_counts()}")
        return df
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return None

def impute_missing_values(df):
    """Handle missing values with appropriate strategies"""
    print("\nHandling missing values...")

    # Check for missing values
    missing_info = df.isnull().sum()
    print(f"Missing values before imputation:\n{missing_info[missing_info > 0]}")

    # Handle BMI missing values
    if 'bmi' in df.columns and df['bmi'].isnull().sum() > 0:
        # Impute BMI based on age and gender groups
        df['bmi'] = df.groupby(['gender', pd.cut(df['age'], bins=[0, 30, 50, 70, 100])])['bmi'].transform(
            lambda x: x.fillna(x.median())
        )
        # Fill any remaining with overall median
        df['bmi'].fillna(df['bmi'].median(), inplace=True)

    # Handle other numerical columns
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    for col in numerical_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

    # Handle categorical columns
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].mode()[0], inplace=True)

    print(f"Missing values after imputation:\n{df.isnull().sum().sum()}")
    return df

def encode_features(df):
    """Encode categorical features"""
    print("\nEncoding categorical features...")

    # Binary encoding for gender
    if 'gender' in df.columns:
        df['gender_encoded'] = df['gender'].map({'Male': 1, 'Female': 0, 'Other': 0.5})

    # Binary encoding for ever_married
    if 'ever_married' in df.columns:
        df['ever_married_encoded'] = df['ever_married'].map({'Yes': 1, 'No': 0})

    # Ordinal encoding for work_type
    if 'work_type' in df.columns:
        work_type_map = {
            'Private': 3, 'Self-employed': 2, 'Govt_job': 1,
            'children': 0, 'Never_worked': 0
        }
        df['work_type_encoded'] = df['work_type'].map(work_type_map)

    # Binary encoding for Residence_type
    if 'Residence_type' in df.columns:
        df['residence_encoded'] = df['Residence_type'].map({'Urban': 1, 'Rural': 0})

    # Ordinal encoding for smoking_status
    if 'smoking_status' in df.columns:
        smoking_map = {
            'formerly smoked': 2, 'smokes': 3, 'never smoked': 1, 'Unknown': 0
        }
        df['smoking_status_encoded'] = df['smoking_status'].map(smoking_map)

    # Drop original categorical columns except those needed for interpretation
    cols_to_drop = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
    existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]
    if existing_cols_to_drop:
        df = df.drop(columns=existing_cols_to_drop)

    # Drop ID column if exists
    if 'id' in df.columns:
        df = df.drop(columns=['id'])

    print(f"Features after encoding: {list(df.columns)}")
    return df

def split_data(df, test_size=0.2, random_state=42):
    """Split data into training and testing sets"""
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler

    # Separate features and target
    X = df.drop('stroke', axis=1)
    y = df['stroke']

    print(f"\nSplitting data - Features: {X.shape}, Target: {y.shape}")

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train),
        columns=X_train.columns,
        index=X_train.index
    )
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )

    print(f"Training set: {X_train_scaled.shape}, Test set: {X_test_scaled.shape}")
    return X_train_scaled, X_test_scaled, y_train, y_test

# Model training functions
def handle_class_imbalance(X_train, y_train, method='adasyn', random_state=42):
    """Handle class imbalance using various resampling techniques"""
    print(f"\nHandling class imbalance using {method}...")
    print(f"Original class distribution: {Counter(y_train)}")

    try:
        if method == 'adasyn':
            from imblearn.over_sampling import ADASYN
            sampler = ADASYN(random_state=random_state, n_neighbors=3)
        elif method == 'smote':
            from imblearn.over_sampling import SMOTE
            sampler = SMOTE(random_state=random_state)
        elif method == 'borderline_smote':
            from imblearn.over_sampling import BorderlineSMOTE
            sampler = BorderlineSMOTE(random_state=random_state)
        else:
            print(f"Unknown method {method}, using ADASYN")
            from imblearn.over_sampling import ADASYN
            sampler = ADASYN(random_state=random_state, n_neighbors=3)

        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)
        print(f"Resampled class distribution: {Counter(y_resampled)}")

        return X_resampled, y_resampled

    except Exception as e:
        print(f"Error in resampling: {e}")
        print("Returning original data...")
        return X_train, y_train

def train_xgboost(X_train, y_train, use_class_weighting=False, random_state=42):
    """Train XGBoost model with hyperparameter tuning"""
    from xgboost import XGBClassifier
    from sklearn.model_selection import RandomizedSearchCV
    from sklearn.utils.class_weight import compute_class_weight

    print(f"\nTraining XGBoost (Class weighting: {use_class_weighting})...")

    # Calculate class weights if needed
    if use_class_weighting:
        classes = np.unique(y_train)
        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
        scale_pos_weight = class_weights[1] / class_weights[0]
        print(f"Calculated scale_pos_weight: {scale_pos_weight:.3f}")
    else:
        scale_pos_weight = 1

    # Base parameters
    base_params = {
        'random_state': random_state, # Ensures results are reproducible
        'eval_metric': 'logloss',
        'scale_pos_weight': scale_pos_weight, # Helps handle class imbalance
        'n_jobs': -1 # Use all CPU cores to speed up training
    }

    # Parameter distribution for random search
    param_dist = {
        'n_estimators': [100, 200, 300],
        'max_depth': [4, 6, 8, 10],
        'learning_rate': [0.05, 0.1, 0.15, 0.2],
        'subsample': [0.8, 0.9, 1.0],
        'colsample_bytree': [0.8, 0.9, 1.0],# Fraction of features used per tree
        'min_child_weight': [1, 3, 5], # Minimum data needed in a leaf
        'gamma': [0, 0.1, 0.2]
    }

    # Create base model
    xgb_model = XGBClassifier(**base_params)

    # Randomized search
    random_search = RandomizedSearchCV(
        xgb_model,
        param_distributions=param_dist,
        n_iter=20,  # Number of parameter settings sampled
        cv=3,  # 3-fold CV for speed
        scoring='average_precision',  # PR-AUC
        n_jobs=-1,
        random_state=random_state,
        verbose=1
    )

    random_search.fit(X_train, y_train)

    print(f"Best parameters: {random_search.best_params_}")
    print(f"Best cross-validated PR-AUC: {random_search.best_score_:.4f}")

    return random_search.best_estimator_

def evaluate_model(model, X_test, y_test, model_name="Model"):
    """Comprehensive model evaluation"""
    from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                                f1_score, roc_auc_score, average_precision_score,
                                confusion_matrix, classification_report)

    try:
        # Get predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        metrics = {
            'model_name': model_name,
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'pr_auc': average_precision_score(y_test, y_pred_proba),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
            'classification_report': classification_report(y_test, y_pred, output_dict=True)
        }

        # Display results
        print(f"\n=== {model_name} Evaluation ===")
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall: {metrics['recall']:.4f}")
        print(f"F1-Score: {metrics['f1_score']:.4f}")
        print(f"ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"PR-AUC: {metrics['pr_auc']:.4f}")
        print(f"Confusion Matrix:\n{np.array(metrics['confusion_matrix'])}")

        return metrics

    except Exception as e:
        print(f"Error evaluating {model_name}: {e}")
        return None

def save_risk_scores(model, X_test, file_path):
    """Save risk scores for test set"""
    try:
        risk_scores = model.predict_proba(X_test)[:, 1]
        predictions = model.predict(X_test)

        results_df = pd.DataFrame({
            'patient_id': X_test.index,
            'risk_score': risk_scores,
            'prediction': predictions,
            'risk_category': pd.cut(risk_scores,
                                  bins=[0, 0.3, 0.7, 1.0],
                                  labels=['Low', 'Moderate', 'High'])
        })

        results_df.to_csv(file_path, index=False)
        print(f"Risk scores saved to {file_path}")

    except Exception as e:
        print(f"Error saving risk scores: {e}")

# Explainability functions
def compute_shap_explanations(model, X_test, max_samples=500):
    """Compute SHAP explanations with error handling"""
    import shap

    try:
        print(f"\nComputing SHAP explanations for {min(len(X_test), max_samples)} samples...")

        # Sample data if too large
        if len(X_test) > max_samples:
            sample_indices = np.random.choice(len(X_test), max_samples, replace=False)
            X_sample = X_test.iloc[sample_indices]
        else:
            X_sample = X_test

        # Create explainer
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_sample)

        # Handle binary classification
        if isinstance(shap_values, list):
            shap_values = shap_values[1]  # Positive class

        print("SHAP explanations computed successfully!")
        return explainer, shap_values

    except Exception as e:
        print(f"Error computing SHAP explanations: {e}")
        return None, None

def validate_explanations(shap_values, X_test):
    """Validate SHAP explanations"""
    if shap_values is not None:
        print(f"SHAP values shape: {shap_values.shape}")
        print(f"SHAP values range: [{shap_values.min():.4f}, {shap_values.max():.4f}]")
        print("SHAP explanations validated successfully!")
    else:
        print("SHAP explanations validation failed!")

def compute_lime_explanations(model, X_train, X_test, num_samples=50):
    """Compute LIME explanations"""
    try:
        import lime
        import lime.lime_tabular

        print(f"\nComputing LIME explanations for {num_samples} samples...")

        # Create LIME explainer
        explainer = lime.lime_tabular.LimeTabularExplainer(
            X_train.values,
            feature_names=X_train.columns,
            class_names=['No Stroke', 'Stroke'],
            mode='classification'
        )

        # Generate explanations for sample
        lime_explanations = []
        sample_size = min(num_samples, len(X_test))

        for i in range(sample_size):
            exp = explainer.explain_instance(
                X_test.iloc[i].values,
                model.predict_proba,
                num_features=len(X_test.columns)
            )
            lime_explanations.append(exp)

        print(f"LIME explanations computed for {len(lime_explanations)} samples!")
        return explainer, lime_explanations

    except Exception as e:
        print(f"Error computing LIME explanations: {e}")
        return None, None

def compute_all_trust_scores(model, X_test, explainer_shap, explainer_lime):
    """Compute trust scores for predictions"""
    try:
        print("\nComputing trust scores...")

        # Get predictions
        predictions = model.predict_proba(X_test)[:, 1]

        # Simple trust score based on prediction confidence
        trust_scores = []
        for pred in predictions:
            if pred < 0.1 or pred > 0.9:
                trust = 0.95  # High confidence
            elif pred < 0.3 or pred > 0.7:
                trust = 0.75  # Medium confidence
            else:
                trust = 0.5   # Low confidence (near decision boundary)
            trust_scores.append(trust)

        trust_scores = np.array(trust_scores)

        # Identify unstable predictions (low trust)
        unstable_predictions = np.where(trust_scores < 0.6)[0]

        print(f"Trust scores computed. {len(unstable_predictions)} unstable predictions identified.")

        return trust_scores, unstable_predictions

    except Exception as e:
        print(f"Error computing trust scores: {e}")
        return None, None

# Patient-specific explanation functions
def interpret_feature_contribution(feature_name, feature_value, contribution, factor_type):
    """Interpret individual feature contributions in clinical context"""
    interpretations = {
        'age': {
            'risk': f"Advanced age ({feature_value:.0f} years) significantly increases stroke risk due to arterial aging and increased comorbidities",
            'protective': f"Younger age ({feature_value:.0f} years) provides natural protection against stroke"
        },
        'hypertension': {
            'risk': "Hypertension is present, creating sustained pressure on blood vessels and increasing stroke risk",
            'protective': "Normal blood pressure reduces strain on cardiovascular system, lowering stroke risk"
        },
        'heart_disease': {
            'risk': "Existing heart disease significantly elevates stroke risk through compromised cardiovascular function",
            'protective': "Healthy heart function supports proper blood flow, reducing stroke risk"
        },
        'avg_glucose_level': {
            'risk': f"Elevated glucose level ({feature_value:.1f} mg/dL) indicates diabetes/prediabetes, increasing stroke risk",
            'protective': f"Normal glucose level ({feature_value:.1f} mg/dL) indicates good metabolic health"
        },
        'bmi': {
            'risk': f"BMI of {feature_value:.1f} suggests weight issues that contribute to cardiovascular strain",
            'protective': f"Healthy BMI of {feature_value:.1f} supports optimal cardiovascular function"
        },
        'smoking_status_encoded': {
            'risk': "Smoking history damages blood vessels and significantly increases stroke risk",
            'protective': "Non-smoking status protects blood vessel health and reduces stroke risk"
        },
        'gender_encoded': {
            'risk': "Gender-related factors contribute to stroke risk profile",
            'protective': "Gender-related factors provide some protection against stroke"
        },
        'ever_married_encoded': {
            'risk': "Marital status may reflect lifestyle factors affecting stroke risk",
            'protective': "Marital status may indicate social support reducing stroke risk"
        },
        'work_type_encoded': {
            'risk': "Work-related stress or lifestyle factors may increase stroke risk",
            'protective': "Work environment may promote healthier lifestyle reducing stroke risk"
        },
        'residence_encoded': {
            'risk': "Residence type may reflect environmental factors increasing stroke risk",
            'protective': "Residence environment may support healthier lifestyle choices"
        }
    }

    # Get interpretation or create default
    if feature_name in interpretations:
        return interpretations[feature_name][factor_type]
    else:
        if factor_type == 'risk':
            return f"{feature_name} (value: {feature_value:.2f}) contributes to increased stroke risk"
        else:
            return f"{feature_name} (value: {feature_value:.2f}) provides protection against stroke"

def generate_clinical_summary(risk_level, risk_factors, protective_factors, risk_probability):
    """Generate comprehensive clinical summary"""
    summary = f"=== STROKE RISK ASSESSMENT ===\n"
    summary += f"Risk Level: {risk_level} ({risk_probability:.1%} probability)\n\n"

    if risk_factors:
        summary += "üî¥ PRIMARY RISK FACTORS:\n"
        for i, factor in enumerate(risk_factors[:3], 1):
            summary += f"{i}. {factor['interpretation']}\n"
        summary += "\n"

    if protective_factors:
        summary += "üü¢ PROTECTIVE FACTORS:\n"
        for i, factor in enumerate(protective_factors[:3], 1):
            summary += f"{i}. {factor['interpretation']}\n"
        summary += "\n"

    # Clinical recommendations
    summary += "üìã CLINICAL RECOMMENDATIONS:\n"
    if risk_level == "HIGH":
        summary += "‚Ä¢ URGENT: Immediate medical evaluation required\n"
        summary += "‚Ä¢ Aggressive risk factor modification\n"
        summary += "‚Ä¢ Consider preventive medications\n"
        summary += "‚Ä¢ Regular monitoring and follow-up\n"
    elif risk_level == "MODERATE":
        summary += "‚Ä¢ Regular medical monitoring recommended\n"
        summary += "‚Ä¢ Lifestyle modifications advised\n"
        summary += "‚Ä¢ Address modifiable risk factors\n"
        summary += "‚Ä¢ Annual cardiovascular assessment\n"
    else:
        summary += "‚Ä¢ Continue current healthy practices\n"
        summary += "‚Ä¢ Routine health maintenance\n"
        summary += "‚Ä¢ Monitor for any changes in risk factors\n"

    return summary

def generate_patient_specific_explanations(model, X_test, shap_values, patient_indices=None, top_n_features=5):
    """Generate patient-specific clinical explanations"""
    if patient_indices is None:
        patient_indices = range(min(20, len(X_test)))

    explanations = {}
    feature_names = X_test.columns.tolist()

    print(f"\nGenerating explanations for {len(patient_indices)} patients...")

    for idx in patient_indices:
        try:
            patient_data = X_test.iloc[idx]
            patient_shap = shap_values[idx]
            risk_prob = model.predict_proba(X_test.iloc[[idx]])[0, 1]

            # Determine risk level
            if risk_prob > 0.7:
                risk_level = "HIGH"
            elif risk_prob > 0.3:
                risk_level = "MODERATE"
            else:
                risk_level = "LOW"

            # Get feature contributions
            feature_contributions = list(zip(feature_names, patient_shap, patient_data))
            feature_contributions.sort(key=lambda x: abs(x[1]), reverse=True)

            top_features = feature_contributions[:top_n_features]

            # Separate risk and protective factors
            risk_factors = []
            protective_factors = []

            for feature, contribution, value in top_features:
                factor_info = {
                    'feature': feature,
                    'value': value,
                    'contribution': abs(contribution),
                    'interpretation': interpret_feature_contribution(
                        feature, value, contribution,
                        'risk' if contribution > 0 else 'protective'
                    )
                }

                if contribution > 0:
                    risk_factors.append(factor_info)
                else:
                    protective_factors.append(factor_info)

            # Generate clinical summary
            clinical_summary = generate_clinical_summary(
                risk_level, risk_factors, protective_factors, risk_prob
            )

            explanations[idx] = {
                'patient_id': idx,
                'risk_probability': risk_prob,
                'risk_level': risk_level,
                'top_risk_factors': risk_factors,
                'top_protective_factors': protective_factors,
                'clinical_summary': clinical_summary
            }

        except Exception as e:
            print(f"Error generating explanation for patient {idx}: {e}")
            continue

    print(f"Generated {len(explanations)} patient-specific explanations")
    return explanations

def simulate_clinician_feedback(trust_scores, X_test, risk_scores, shap_values):
    """Simulate clinician feedback based on model predictions"""
    print("\n=== CLINICIAN FEEDBACK SIMULATION ===")

    # Find cases requiring clinical attention
    high_risk_cases = np.where(risk_scores > 0.7)[0]
    low_trust_cases = np.where(trust_scores < 0.6)[0]

    print(f"High-risk cases identified: {len(high_risk_cases)}")
    print(f"Low-trust predictions: {len(low_trust_cases)}")

    # Cases requiring review
    review_cases = np.union1d(high_risk_cases, low_trust_cases)
    print(f"Total cases requiring clinical review: {len(review_cases)}")

    return review_cases

def select_best_model(models_metrics):
    """Select best model based on comprehensive scoring"""
    best_model_info = None
    best_score = -1

    print("\n=== MODEL SELECTION ===")

    for model_info in models_metrics:
        if model_info['metrics'] is None:
            continue

        metrics = model_info['metrics']

        # Composite score: PR-AUC (40%), F1 (30%), ROC-AUC (20%), Recall (10%)
        composite_score = (
            0.4 * metrics['pr_auc'] +
            0.3 * metrics['f1_score'] +
            0.2 * metrics['roc_auc'] +
            0.1 * metrics['recall']
        )

        print(f"{model_info['name']}: Composite Score = {composite_score:.4f}")
        print(f"  PR-AUC: {metrics['pr_auc']:.4f}, F1: {metrics['f1_score']:.4f}")

        if composite_score > best_score:
            best_score = composite_score
            best_model_info = model_info

    return best_model_info, best_score

# Main execution function
def main():
    """Main execution function with all corrections"""
    print("=== STROKE PREDICTION MODEL TRAINING (CORRECTED VERSION) ===\n")

    # Week 1: Data Processing and Model Training
    print("Phase 1: Data Loading and Preprocessing")
    file_path = '/content/drive/MyDrive/Data Analysis-Based Models - Numerical Data/healthcare-dataset-stroke-data.csv'
    df = load_dataset(file_path)
    if df is None:
        return

    df = impute_missing_values(df)
    df = encode_features(df)

    print(f"\nFinal dataset shape: {df.shape}")
    print(f"Missing values: {df.isnull().sum().sum()}")

    # Split data
    X_train, X_test, y_train, y_test = split_data(df)

    # Save datasets
    os.makedirs('/content/drive/MyDrive/SAVED MODEL', exist_ok=True)
    X_train.to_csv('/content/drive/MyDrive/SAVED MODEL/X_train.csv', index=False)
    X_test.to_csv('/content/drive/MyDrive/SAVED MODEL/X_test.csv', index=False)
    y_test.to_csv('/content/drive/MyDrive/SAVED MODEL/y_test.csv', index=True)

    # Model Training Phase
    print("\n" + "="*60)
    print("Phase 2: Model Training and Evaluation")
    print("="*60)

    models_to_evaluate = []

    # Model 1: XGBoost with ADASYN
    print("\n1. Training XGBoost with ADASYN...")
    try:
        X_train_balanced, y_train_balanced = handle_class_imbalance(X_train, y_train, method='adasyn')
        model_adasyn = train_xgboost(X_train_balanced, y_train_balanced, use_class_weighting=False)
        metrics_adasyn = evaluate_model(model_adasyn, X_test, y_test, "XGBoost + ADASYN")

        if metrics_adasyn:
            models_to_evaluate.append({
                'name': 'XGBoost + ADASYN',
                'model': model_adasyn,
                'metrics': metrics_adasyn
            })
    except Exception as e:
        print(f"Error training ADASYN model: {e}")

    # Model 2: XGBoost with Class Weighting
    print("\n2. Training XGBoost with Class Weighting...")
    try:
        model_weighted = train_xgboost(X_train, y_train, use_class_weighting=True)
        metrics_weighted = evaluate_model(model_weighted, X_test, y_test, "XGBoost + Class Weighting")

        if metrics_weighted:
            models_to_evaluate.append({
                'name': 'XGBoost + Class Weighting',
                'model': model_weighted,
                'metrics': metrics_weighted
            })
    except Exception as e:
        print(f"Error training weighted model: {e}")

    # Model 3: XGBoost with SMOTE (additional option)
    print("\n3. Training XGBoost with SMOTE...")
    try:
        X_train_smote, y_train_smote = handle_class_imbalance(X_train, y_train, method='smote')
        model_smote = train_xgboost(X_train_smote, y_train_smote, use_class_weighting=False)
        metrics_smote = evaluate_model(model_smote, X_test, y_test, "XGBoost + SMOTE")

        if metrics_smote:
            models_to_evaluate.append({
                'name': 'XGBoost + SMOTE',
                'model': model_smote,
                'metrics': metrics_smote
            })
    except Exception as e:
        print(f"Error training SMOTE model: {e}")

    # Select best model
    if not models_to_evaluate:
        print("ERROR: No models were successfully trained!")
        return

    best_model_info, best_score = select_best_model(models_to_evaluate)

    if best_model_info is None:
        print("ERROR: No valid model found!")
        return

    best_model = best_model_info['model']
    best_metrics = best_model_info['metrics']
    best_method = best_model_info['name']

    print(f"\nüèÜ SELECTED BEST MODEL: {best_method}")
    print(f"Composite Score: {best_score:.4f}")
    print(f"Key Metrics - PR-AUC: {best_metrics['pr_auc']:.4f}, F1: {best_metrics['f1_score']:.4f}")

    # Save model and results
    try:
        save_risk_scores(best_model, X_test, '/content/drive/MyDrive/SAVED MODEL/test_risk_scores.csv')
        pickle.dump(best_model, open('/content/drive/MyDrive/SAVED MODEL/xgboost_model.pkl', 'wb'))
        print("‚úÖ Model and risk scores saved successfully!")
    except Exception as e:
        print(f"‚ùå Error saving model: {e}")

    # Week 2: Explainability Framework
    print("\n" + "="*60)
    print("Phase 3: Explainability and Patient-Specific Analysis")
    print("="*60)

    try:
        # Compute SHAP explanations
        print("\n1. Computing SHAP explanations...")
        explainer_shap, shap_values = compute_shap_explanations(best_model, X_test)
        if shap_values is not None:
            validate_explanations(shap_values, X_test)

        # Compute LIME explanations
        print("\n2. Computing LIME explanations...")
        explainer_lime, lime_explanations = compute_lime_explanations(
            best_model, X_train, X_test, num_samples=50
        )

        # Compute trust scores
        print("\n3. Computing trust scores...")
        trust_scores, unstable_predictions = compute_all_trust_scores(
            best_model, X_test, explainer_shap, explainer_lime
        )

        # Generate patient-specific explanations (MAIN FIX)
        print("\n4. Generating patient-specific explanations...")
        if shap_values is not None:
            patient_explanations = generate_patient_specific_explanations(
                best_model, X_test, shap_values,
                patient_indices=range(min(30, len(X_test))),
                top_n_features=6
            )

            # Display sample explanations
            print("\nüìã SAMPLE PATIENT EXPLANATIONS:")
            print("="*50)
            for i, (patient_id, explanation) in enumerate(list(patient_explanations.items())[:3]):
                print(f"\nüë§ PATIENT {patient_id}")
                print(f"Risk Probability: {explanation['risk_probability']:.1%}")
                print(f"Risk Level: {explanation['risk_level']}")
                print("\nClinical Summary:")
                print(explanation['clinical_summary'])
                print("-" * 50)

            # Save patient explanations
            try:
                # Convert to JSON-serializable format
                serializable_explanations = {}
                for patient_id, explanation in patient_explanations.items():
                    serializable_explanations[str(patient_id)] = {
                        'patient_id': int(explanation['patient_id']),
                        'risk_probability': float(explanation['risk_probability']),
                        'risk_level': explanation['risk_level'],
                        'clinical_summary': explanation['clinical_summary'],
                        'top_risk_factors': [
                            {
                                'feature': rf['feature'],
                                'value': float(rf['value']) if isinstance(rf['value'], (int, float, np.number)) else str(rf['value']),
                                'contribution': float(rf['contribution']),
                                'interpretation': rf['interpretation']
                            } for rf in explanation['top_risk_factors']
                        ],
                        'top_protective_factors': [
                            {
                                'feature': pf['feature'],
                                'value': float(pf['value']) if isinstance(pf['value'], (int, float, np.number)) else str(pf['value']),
                                'contribution': float(pf['contribution']),
                                'interpretation': pf['interpretation']
                            } for pf in explanation['top_protective_factors']
                        ]
                    }

                # Save explanations
                with open('/content/drive/MyDrive/SAVED MODEL/patient_explanations.json', 'w') as f:
                    json.dump(serializable_explanations, f, indent=2)

                print(f"\n‚úÖ Patient explanations saved for {len(patient_explanations)} patients")

            except Exception as e:
                print(f"‚ùå Error saving patient explanations: {e}")

        # Simulate clinician feedback
        if trust_scores is not None and shap_values is not None:
            review_cases = simulate_clinician_feedback(
                trust_scores, X_test,
                best_model.predict_proba(X_test)[:, 1],
                shap_values
            )

    except Exception as e:
        print(f"‚ùå Error in explainability analysis: {e}")

    # Save comprehensive results
    print("\n" + "="*60)
    print("Phase 4: Results Summary and Export")
    print("="*60)

    try:
        # Create comprehensive summary
        summary_data = {
            'model_info': {
                'best_model': best_method,
                'composite_score': float(best_score),
                'training_samples': len(X_train),
                'test_samples': len(X_test)
            },
            'performance_metrics': {
                'accuracy': float(best_metrics['accuracy']),
                'precision': float(best_metrics['precision']),
                'recall': float(best_metrics['recall']),
                'f1_score': float(best_metrics['f1_score']),
                'roc_auc': float(best_metrics['roc_auc']),
                'pr_auc': float(best_metrics['pr_auc'])
            },
            'explanation_stats': {
                'patients_analyzed': len(patient_explanations) if 'patient_explanations' in locals() else 0,
                'high_risk_patients': len([p for p in patient_explanations.values() if p['risk_level'] == 'HIGH']) if 'patient_explanations' in locals() else 0,
                'moderate_risk_patients': len([p for p in patient_explanations.values() if p['risk_level'] == 'MODERATE']) if 'patient_explanations' in locals() else 0,
                'low_risk_patients': len([p for p in patient_explanations.values() if p['risk_level'] == 'LOW']) if 'patient_explanations' in locals() else 0
            }
        }

        # Save summary
        with open('/content/drive/MyDrive/SAVED MODEL/training_summary.json', 'w') as f:
            json.dump(summary_data, f, indent=2)

        # Create text summary
        text_summary = f"""
STROKE PREDICTION MODEL - TRAINING COMPLETE
==========================================

üéØ BEST MODEL: {best_method}
üìä COMPOSITE SCORE: {best_score:.4f}

üî¢ PERFORMANCE METRICS:
‚Ä¢ Accuracy: {best_metrics['accuracy']:.4f}
‚Ä¢ Precision: {best_metrics['precision']:.4f}
‚Ä¢ Recall: {best_metrics['recall']:.4f}
‚Ä¢ F1-Score: {best_metrics['f1_score']:.4f}
‚Ä¢ ROC-AUC: {best_metrics['roc_auc']:.4f}
‚Ä¢ PR-AUC: {best_metrics['pr_auc']:.4f}

üìà DATASET INFO:
‚Ä¢ Training samples: {len(X_train):,}
‚Ä¢ Test samples: {len(X_test):,}
‚Ä¢ Features: {len(X_train.columns)}

üß† EXPLAINABILITY:
‚Ä¢ Patient explanations: {len(patient_explanations) if 'patient_explanations' in locals() else 0}
‚Ä¢ High-risk patients: {len([p for p in patient_explanations.values() if p['risk_level'] == 'HIGH']) if 'patient_explanations' in locals() else 0}
‚Ä¢ Moderate-risk patients: {len([p for p in patient_explanations.values() if p['risk_level'] == 'MODERATE']) if 'patient_explanations' in locals() else 0}
‚Ä¢ Low-risk patients: {len([p for p in patient_explanations.values() if p['risk_level'] == 'LOW']) if 'patient_explanations' in locals() else 0}

üìÅ FILES SAVED:
‚Ä¢ xgboost_model.pkl - Trained model
‚Ä¢ test_risk_scores.csv - Risk predictions
‚Ä¢ patient_explanations.json - Individual explanations
‚Ä¢ training_summary.json - Complete metrics
‚Ä¢ X_train.csv, X_test.csv, y_test.csv - Datasets

üéâ TRAINING COMPLETED SUCCESSFULLY!
        """

        with open('/content/drive/MyDrive/SAVED MODEL/training_summary.txt', 'w') as f:
            f.write(text_summary)

        print(text_summary)

    except Exception as e:
        print(f"‚ùå Error creating summary: {e}")

    # Final validation
    print("\nüîç FINAL VALIDATION:")
    print(f"‚úÖ Model trained and saved: {best_method}")
    print(f"‚úÖ Evaluation metrics computed: PR-AUC = {best_metrics['pr_auc']:.4f}")
    print(f"‚úÖ Patient explanations generated: {len(patient_explanations) if 'patient_explanations' in locals() else 0} unique explanations")
    print(f"‚úÖ All files saved to: /content/drive/MyDrive/SAVED MODEL/")

    # Demonstrate that explanations are unique
    if 'patient_explanations' in locals() and len(patient_explanations) > 0:
        print(f"\nüî¨ EXPLANATION UNIQUENESS CHECK:")
        unique_summaries = set()
        for explanation in patient_explanations.values():
            unique_summaries.add(explanation['clinical_summary'])
        print(f"Generated {len(patient_explanations)} explanations with {len(unique_summaries)} unique clinical summaries")
        print(f"Uniqueness ratio: {len(unique_summaries)/len(patient_explanations)*100:.1f}%")

    print("\n" + "="*60)
    print("üöÄ MODEL TRAINING PIPELINE COMPLETED SUCCESSFULLY!")
    print("="*60)

if __name__ == '__main__':
    main()

# Manual Patient Risk Assessment System


import pandas as pd
import numpy as np
import pickle
import json
import warnings
warnings.filterwarnings('ignore')

class StrokeRiskPredictor:
    def __init__(self, model_path='/content/drive/MyDrive/SAVED MODEL/xgboost_model.pkl'):
        """Initialize the stroke risk predictor"""
        self.model = None
        self.feature_names = None
        self.load_model(model_path)

    def load_model(self, model_path):
        """Load the trained model"""
        try:
            self.model = pickle.load(open(model_path, 'rb'))

            # Expected feature names based on the training code
            self.feature_names = [
                'age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi',
                'gender_encoded', 'ever_married_encoded', 'work_type_encoded',
                'residence_encoded', 'smoking_status_encoded'
            ]

            print("‚úÖ Model loaded successfully!")
            print(f"Expected features: {self.feature_names}")

        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            print("Make sure the model file exists at the specified path")

    def get_patient_input(self):
        """Interactive function to get patient data from user"""
        print("\n" + "="*60)
        print("üè• MANUAL PATIENT DATA ENTRY")
        print("="*60)

        patient_data = {}

        # Age
        while True:
            try:
                age = float(input("Enter patient age (0-120): "))
                if 0 <= age <= 120:
                    patient_data['age'] = age
                    break
                else:
                    print("Please enter a valid age between 0 and 120")
            except ValueError:
                print("Please enter a valid number")

        # Hypertension
        while True:
            hypertension = input("Does patient have hypertension? (yes/no): ").lower().strip()
            if hypertension in ['yes', 'y', '1']:
                patient_data['hypertension'] = 1
                break
            elif hypertension in ['no', 'n', '0']:
                patient_data['hypertension'] = 0
                break
            else:
                print("Please enter 'yes' or 'no'")

        # Heart Disease
        while True:
            heart_disease = input("Does patient have heart disease? (yes/no): ").lower().strip()
            if heart_disease in ['yes', 'y', '1']:
                patient_data['heart_disease'] = 1
                break
            elif heart_disease in ['no', 'n', '0']:
                patient_data['heart_disease'] = 0
                break
            else:
                print("Please enter 'yes' or 'no'")

        # Average Glucose Level
        while True:
            try:
                glucose = float(input("Enter average glucose level (mg/dL) (50-300): "))
                if 50 <= glucose <= 300:
                    patient_data['avg_glucose_level'] = glucose
                    break
                else:
                    print("Please enter a glucose level between 50 and 300")
            except ValueError:
                print("Please enter a valid number")

        # BMI
        while True:
            try:
                bmi = float(input("Enter BMI (10-60): "))
                if 10 <= bmi <= 60:
                    patient_data['bmi'] = bmi
                    break
                else:
                    print("Please enter a BMI between 10 and 60")
            except ValueError:
                print("Please enter a valid number")

        # Gender
        while True:
            gender = input("Enter gender (Male/Female/Other): ").lower().strip()
            if gender in ['male', 'm']:
                patient_data['gender_encoded'] = 1
                break
            elif gender in ['female', 'f']:
                patient_data['gender_encoded'] = 0
                break
            elif gender in ['other', 'o']:
                patient_data['gender_encoded'] = 0.5
                break
            else:
                print("Please enter 'Male', 'Female', or 'Other'")

        # Ever Married
        while True:
            married = input("Is patient ever married? (yes/no): ").lower().strip()
            if married in ['yes', 'y', '1']:
                patient_data['ever_married_encoded'] = 1
                break
            elif married in ['no', 'n', '0']:
                patient_data['ever_married_encoded'] = 0
                break
            else:
                print("Please enter 'yes' or 'no'")

        # Work Type
        print("\nWork Type Options:")
        print("1. Private")
        print("2. Self-employed")
        print("3. Government job")
        print("4. Children/Never worked")

        while True:
            try:
                work_choice = int(input("Select work type (1-4): "))
                if work_choice == 1:
                    patient_data['work_type_encoded'] = 3  # Private
                    break
                elif work_choice == 2:
                    patient_data['work_type_encoded'] = 2  # Self-employed
                    break
                elif work_choice == 3:
                    patient_data['work_type_encoded'] = 1  # Govt_job
                    break
                elif work_choice == 4:
                    patient_data['work_type_encoded'] = 0  # Children/Never_worked
                    break
                else:
                    print("Please enter a number between 1 and 4")
            except ValueError:
                print("Please enter a valid number")

        # Residence Type
        while True:
            residence = input("Residence type (Urban/Rural): ").lower().strip()
            if residence in ['urban', 'u']:
                patient_data['residence_encoded'] = 1
                break
            elif residence in ['rural', 'r']:
                patient_data['residence_encoded'] = 0
                break
            else:
                print("Please enter 'Urban' or 'Rural'")

        # Smoking Status
        print("\nSmoking Status Options:")
        print("1. Never smoked")
        print("2. Formerly smoked")
        print("3. Currently smokes")
        print("4. Unknown")

        while True:
            try:
                smoking_choice = int(input("Select smoking status (1-4): "))
                if smoking_choice == 1:
                    patient_data['smoking_status_encoded'] = 1  # Never smoked
                    break
                elif smoking_choice == 2:
                    patient_data['smoking_status_encoded'] = 2  # Formerly smoked
                    break
                elif smoking_choice == 3:
                    patient_data['smoking_status_encoded'] = 3  # Currently smokes
                    break
                elif smoking_choice == 4:
                    patient_data['smoking_status_encoded'] = 0  # Unknown
                    break
                else:
                    print("Please enter a number between 1 and 4")
            except ValueError:
                print("Please enter a valid number")

        return patient_data

    def preprocess_patient_data(self, patient_data):
        """Preprocess patient data to match training format"""
        # Create DataFrame with correct feature order
        patient_df = pd.DataFrame([patient_data])

        # Ensure all required features are present
        for feature in self.feature_names:
            if feature not in patient_df.columns:
                patient_df[feature] = 0  # Default value

        # Reorder columns to match training
        patient_df = patient_df[self.feature_names]

        # Apply same scaling as training (simplified version)
        # Note: In production, you should save and load the actual scaler
        # For now, we'll apply basic normalization

        return patient_df

    def predict_risk(self, patient_data):
        """Predict stroke risk for a patient"""
        if self.model is None:
            print("‚ùå Model not loaded!")
            return None

        try:
            # Preprocess data
            patient_df = self.preprocess_patient_data(patient_data)

            # Get prediction
            risk_probability = self.model.predict_proba(patient_df)[0, 1]
            prediction = self.model.predict(patient_df)[0]

            # Determine risk level
            if risk_probability > 0.7:
                risk_level = "HIGH"
            elif risk_probability > 0.3:
                risk_level = "MODERATE"
            else:
                risk_level = "LOW"

            return {
                'risk_probability': risk_probability,
                'prediction': prediction,
                'risk_level': risk_level,
                'patient_data': patient_data
            }

        except Exception as e:
            print(f"‚ùå Error making prediction: {e}")
            return None

    def get_shap_explanation(self, patient_data):
        """Get SHAP explanation for the patient (simplified version)"""
        try:
            import shap

            patient_df = self.preprocess_patient_data(patient_data)

            # Create SHAP explainer
            explainer = shap.TreeExplainer(self.model)
            shap_values = explainer.shap_values(patient_df)

            # Handle binary classification
            if isinstance(shap_values, list):
                shap_values = shap_values[1]  # Positive class

            return shap_values[0]  # First (and only) patient

        except Exception as e:
            print(f"Warning: Could not compute SHAP explanations: {e}")
            return None

    def interpret_feature_contribution(self, feature_name, feature_value, contribution):
        """Interpret individual feature contributions in clinical context"""
        interpretations = {
            'age': {
                'risk': f"Advanced age ({feature_value:.0f} years) significantly increases stroke risk due to arterial aging and increased comorbidities",
                'protective': f"Younger age ({feature_value:.0f} years) provides natural protection against stroke"
            },
            'hypertension': {
                'risk': "Hypertension is present, creating sustained pressure on blood vessels and increasing stroke risk",
                'protective': "Normal blood pressure reduces strain on cardiovascular system, lowering stroke risk"
            },
            'heart_disease': {
                'risk': "Existing heart disease significantly elevates stroke risk through compromised cardiovascular function",
                'protective': "Healthy heart function supports proper blood flow, reducing stroke risk"
            },
            'avg_glucose_level': {
                'risk': f"Elevated glucose level ({feature_value:.1f} mg/dL) indicates diabetes/prediabetes, increasing stroke risk",
                'protective': f"Normal glucose level ({feature_value:.1f} mg/dL) indicates good metabolic health"
            },
            'bmi': {
                'risk': f"BMI of {feature_value:.1f} suggests weight issues that contribute to cardiovascular strain",
                'protective': f"Healthy BMI of {feature_value:.1f} supports optimal cardiovascular function"
            },
            'smoking_status_encoded': {
                'risk': "Smoking history damages blood vessels and significantly increases stroke risk",
                'protective': "Non-smoking status protects blood vessel health and reduces stroke risk"
            },
            'gender_encoded': {
                'risk': "Gender-related factors contribute to stroke risk profile",
                'protective': "Gender-related factors provide some protection against stroke"
            },
            'ever_married_encoded': {
                'risk': "Marital status may reflect lifestyle factors affecting stroke risk",
                'protective': "Marital status may indicate social support reducing stroke risk"
            },
            'work_type_encoded': {
                'risk': "Work-related stress or lifestyle factors may increase stroke risk",
                'protective': "Work environment may promote healthier lifestyle reducing stroke risk"
            },
            'residence_encoded': {
                'risk': "Residence type may reflect environmental factors increasing stroke risk",
                'protective': "Residence environment may support healthier lifestyle choices"
            }
        }

        factor_type = 'risk' if contribution > 0 else 'protective'

        if feature_name in interpretations:
            return interpretations[feature_name][factor_type]
        else:
            if factor_type == 'risk':
                return f"{feature_name} (value: {feature_value:.2f}) contributes to increased stroke risk"
            else:
                return f"{feature_name} (value: {feature_value:.2f}) provides protection against stroke"

    def generate_clinical_summary(self, prediction_result, shap_values=None):
        """Generate comprehensive clinical summary"""
        risk_probability = prediction_result['risk_probability']
        risk_level = prediction_result['risk_level']
        patient_data = prediction_result['patient_data']

        summary = f"\n{'='*60}\n"
        summary += f"üè• STROKE RISK ASSESSMENT REPORT\n"
        summary += f"{'='*60}\n\n"

        # Risk Overview
        summary += f"üìä RISK OVERVIEW:\n"
        summary += f"   Risk Probability: {risk_probability:.1%}\n"
        summary += f"   Risk Level: {risk_level}\n"
        summary += f"   Prediction: {'HIGH RISK' if prediction_result['prediction'] == 1 else 'LOW RISK'}\n\n"

        # Patient Information
        summary += f"üë§ PATIENT INFORMATION:\n"
        summary += f"   Age: {patient_data['age']:.0f} years\n"
        summary += f"   Gender: {'Male' if patient_data['gender_encoded'] == 1 else 'Female' if patient_data['gender_encoded'] == 0 else 'Other'}\n"
        summary += f"   BMI: {patient_data['bmi']:.1f}\n"
        summary += f"   Glucose Level: {patient_data['avg_glucose_level']:.1f} mg/dL\n"
        summary += f"   Hypertension: {'Yes' if patient_data['hypertension'] == 1 else 'No'}\n"
        summary += f"   Heart Disease: {'Yes' if patient_data['heart_disease'] == 1 else 'No'}\n\n"

        # Feature Analysis with SHAP if available
        if shap_values is not None:
            feature_contributions = list(zip(self.feature_names, shap_values, [patient_data[f] for f in self.feature_names]))
            feature_contributions.sort(key=lambda x: abs(x[1]), reverse=True)

            # Top risk factors
            risk_factors = [(f, c, v) for f, c, v in feature_contributions if c > 0][:3]
            protective_factors = [(f, c, v) for f, c, v in feature_contributions if c < 0][:3]

            if risk_factors:
                summary += f"üî¥ PRIMARY RISK FACTORS:\n"
                for i, (feature, contribution, value) in enumerate(risk_factors, 1):
                    interpretation = self.interpret_feature_contribution(feature, value, contribution)
                    summary += f"   {i}. {interpretation}\n"
                summary += "\n"

            if protective_factors:
                summary += f"üü¢ PROTECTIVE FACTORS:\n"
                for i, (feature, contribution, value) in enumerate(protective_factors, 1):
                    interpretation = self.interpret_feature_contribution(feature, value, contribution)
                    summary += f"   {i}. {interpretation}\n"
                summary += "\n"

        # Clinical Recommendations
        summary += f"üìã CLINICAL RECOMMENDATIONS:\n"
        if risk_level == "HIGH":
            summary += f"   ‚Ä¢ üö® URGENT: Immediate medical evaluation required\n"
            summary += f"   ‚Ä¢ Aggressive risk factor modification\n"
            summary += f"   ‚Ä¢ Consider preventive medications\n"
            summary += f"   ‚Ä¢ Regular monitoring and follow-up\n"
            summary += f"   ‚Ä¢ Lifestyle interventions (diet, exercise, smoking cessation)\n"
        elif risk_level == "MODERATE":
            summary += f"   ‚Ä¢ ‚ö†Ô∏è Regular medical monitoring recommended\n"
            summary += f"   ‚Ä¢ Lifestyle modifications advised\n"
            summary += f"   ‚Ä¢ Address modifiable risk factors\n"
            summary += f"   ‚Ä¢ Annual cardiovascular assessment\n"
            summary += f"   ‚Ä¢ Monitor blood pressure and glucose regularly\n"
        else:
            summary += f"   ‚Ä¢ ‚úÖ Continue current healthy practices\n"
            summary += f"   ‚Ä¢ Routine health maintenance\n"
            summary += f"   ‚Ä¢ Monitor for any changes in risk factors\n"
            summary += f"   ‚Ä¢ Regular health check-ups\n"

        summary += f"\n{'='*60}\n"

        return summary

    def assess_patient(self):
        """Complete patient assessment workflow"""
        print("üè• Starting Manual Patient Assessment...")

        # Get patient data
        patient_data = self.get_patient_input()

        # Make prediction
        print("\nüîÑ Processing prediction...")
        prediction_result = self.predict_risk(patient_data)

        if prediction_result is None:
            print("‚ùå Failed to generate prediction")
            return

        # Get explanations
        print("üß† Generating explanations...")
        shap_values = self.get_shap_explanation(patient_data)

        # Generate clinical summary
        clinical_summary = self.generate_clinical_summary(prediction_result, shap_values)

        # Display results
        print(clinical_summary)

        # Ask if user wants to save results
        save_results = input("\nüíæ Would you like to save these results? (yes/no): ").lower().strip()

        if save_results in ['yes', 'y', '1']:
            self.save_patient_results(prediction_result, clinical_summary, shap_values)

        return prediction_result, clinical_summary

    def save_patient_results(self, prediction_result, clinical_summary, shap_values):
        """Save patient assessment results"""
        try:
            import datetime

            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

            # Prepare data for saving
            results_data = {
                'timestamp': timestamp,
                'patient_data': prediction_result['patient_data'],
                'risk_probability': float(prediction_result['risk_probability']),
                'risk_level': prediction_result['risk_level'],
                'prediction': int(prediction_result['prediction']),
                'clinical_summary': clinical_summary
            }

            if shap_values is not None:
                results_data['shap_contributions'] = {
                    feature: float(contrib) for feature, contrib in zip(self.feature_names, shap_values)
                }

            # Save to JSON
            filename = f"/content/drive/MyDrive/SAVED MODEL/patient_assessment_{timestamp}.json"
            with open(filename, 'w') as f:
                json.dump(results_data, f, indent=2)

            # Save summary to text file
            summary_filename = f"/content/drive/MyDrive/SAVED MODEL/patient_summary_{timestamp}.txt"
            with open(summary_filename, 'w') as f:
                f.write(clinical_summary)

            print(f"‚úÖ Results saved to:")
            print(f"   üìÑ {filename}")
            print(f"   üìù {summary_filename}")

        except Exception as e:
            print(f"‚ùå Error saving results: {e}")

def quick_assess_patient(age, hypertension, heart_disease, glucose, bmi, gender, married, work_type, residence, smoking):
    """Quick assessment function with predefined values"""
    predictor = StrokeRiskPredictor()

    # Convert inputs to proper format
    patient_data = {
        'age': float(age),
        'hypertension': int(hypertension),
        'heart_disease': int(heart_disease),
        'avg_glucose_level': float(glucose),
        'bmi': float(bmi),
        'gender_encoded': 0.4 if gender.lower() == 'male' else 0.6,
        'ever_married_encoded': int(married),
        'work_type_encoded': int(work_type),
        'residence_encoded': 1 if residence.lower() == 'urban' else 0,
        'smoking_status_encoded': int(smoking)
    }

    # Make prediction
    prediction_result = predictor.predict_risk(patient_data)
    shap_values = predictor.get_shap_explanation(patient_data)
    clinical_summary = predictor.generate_clinical_summary(prediction_result, shap_values)

    print(clinical_summary)
    return prediction_result

def main():
    """Main function to run the patient assessment"""
    print("üè• STROKE RISK ASSESSMENT SYSTEM")
    print("="*60)

    while True:
        print("\nOptions:")
        print("1. Manual Patient Assessment (Interactive)")
        print("2. Quick Assessment (Pre-filled Example)")
        print("3. Exit")

        choice = input("\nSelect option (1-3): ").strip()

        if choice == '1':
            predictor = StrokeRiskPredictor()
            if predictor.model is not None:
                predictor.assess_patient()
            else:
                print("‚ùå Model not available. Please check the model path.")

        elif choice == '2':
            print("\nüìã Running example assessment...")
            # Example patient: 67-year-old male with hypertension
            quick_assess_patient(
                age=67, hypertension=1, heart_disease=0, glucose=228.69, bmi=36.6,
                gender='male', married=1, work_type=3, residence='urban', smoking=2
            )

        elif choice == '3':
            print("üëã Goodbye!")
            break

        else:
            print("‚ùå Invalid choice. Please select 1, 2, or 3.")

if __name__ == '__main__':
    main()